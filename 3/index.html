<!DOCTYPE html>
<html>
<head>
    <title>Project 3: Auto-stitching and Photo Mosaics</title>
    <link rel="stylesheet" href="../styles.css">
</head>
<body class="centered">
    <div class="banner">Project 3: Auto-stitching and Photo Mosaics</div>

    <div class="main-content">
        <div class="card">
            <h2>Overview</h2>
            <p>
                <strong>Part A</strong> implements the fundamental mosaic pipeline using manually selected point correspondences. I compute homography transformations between image pairs, warp images using inverse warping with nearest neighbor and bilinear interpolation, and blend them together using Gaussian-weighted alpha masks for seamless transitions.
            </p>
            <p>
                <strong>Part B</strong> builds on Part A by automating the entire pipeline. Using Harris corner detection, Adaptive Non-Maximal Suppression, and feature descriptors, I automatically identify and match corresponding points between images. RANSAC is then used to robustly estimate homographies despite outliers, producing results comparable to manual methods without requiring user intervention.
            </p>
        </div>

        <div class="card">
            <h2>A.1: Shoot the Pictures</h2>
            <p>
                For this project, I captured multiple sets of photographs with projective transformations between them. The key requirement is to keep the center of projection (COP) fixed while rotating the camera to capture different views. This ensures that the transformations between images are pure homographies.
            </p>
            
            <h3>Image Set Requirements</h3>
            <ul>
                <li><strong>Center of Projection:</strong> Camera rotates around a fixed point (optical center)</li>
                <li><strong>Overlap:</strong> 40-70% overlap between consecutive images for reliable registration</li>
                <li><strong>Timing:</strong> Images captured close together to minimize lighting changes and subject movement</li>
                <li><strong>Lens Considerations:</strong> Avoiding fisheye lenses or significant barrel distortion</li>
            </ul>

            <h3>Captured Image Sets</h3>
            <div class="mosaic-section">
                <h4>Set 1: TV Mosaic</h4>
                <p>This set consists of two images of a TV screen captured by rotating the camera around its center of projection. The images have significant overlap and contain sufficient texture for point correspondence.</p>
                <div class="source-images">
                    <div>
                        <img src="AOutputs/A4_tvmosaic1_original.jpg" alt="TV Mosaic Image 1">
                        <p><strong>Image 1</strong></p>
                    </div>
                    <div>
                        <img src="AOutputs/A4_tvmosaic2_reference.jpg" alt="TV Mosaic Image 2">
                        <p><strong>Image 2 (Reference)</strong></p>
                    </div>
                </div>
            </div>

            <div class="mosaic-section">
                <h4>Set 2: Interior Scene</h4>
                <p>This set captures an interior scene with the camera rotating around a fixed center of projection. The images contain rich texture and detail for reliable point correspondence.</p>
                <div class="source-images">
                    <div>
                        <img src="AOriginals/HM1.jpg" alt="Interior Scene Image 1">
                        <p><strong>Image 1</strong></p>
                    </div>
                    <div>
                        <img src="AOriginals/HM2.jpg" alt="Interior Scene Image 2">
                        <p><strong>Image 2 (Reference)</strong></p>
                    </div>
                </div>
            </div>

            <div class="mosaic-section">
                <h4>Set 3: Facade Mosaic</h4>
                <p>This set captures a building facade with the camera rotating around a fixed center of projection. The images demonstrate projective transformation between views of architectural features.</p>
                <div class="source-images">
                    <div>
                        <img src="AOriginals/FacadeMosaic1.jpg" alt="Facade Mosaic Image 1">
                        <p><strong>Image 1</strong></p>
                    </div>
                    <div>
                        <img src="AOriginals/FacadeMosaic2.jpg" alt="Facade Mosaic Image 2">
                        <p><strong>Image 2 (Reference)</strong></p>
                    </div>
                </div>
            </div>
        </div>

        <div class="card">
            <h2>A.2: Recover Homographies</h2>
            <p>
                A homography is a projective transformation represented by a 3×3 matrix H that maps points from one image to another: p' = Hp. Since the matrix has 8 degrees of freedom (9 elements minus 1 for scale), we need at least 4 point correspondences to solve for H. However, using more than 4 points and solving via least-squares provides more robust results.
            </p>

            <h3>Implementation: computeH(im1_pts, im2_pts)</h3>
            <p>
                Given n pairs of corresponding points (x, y) in image 1 and (x', y') in image 2, I set up a system of linear equations. For each point correspondence, the homography relationship gives us:
            </p>
            
            <div class="equation-box">
                x' = (h11*x + h12*y + h13) / (h31*x + h32*y + h33)
                y' = (h21*x + h22*y + h23) / (h31*x + h32*y + h33)
            </div>

            <p>Rearranging to eliminate the denominator and setting h33 = 1, we get two equations per point correspondence:</p>

            <div class="equation-box">
                h11*x + h12*y + h13 - h31*x*x' - h32*y*x' = x'
                h21*x + h22*y + h23 - h31*x*y' - h32*y*y' = y'
            </div>

            <p>
                This forms a system Ah = b where A is a 2n × 8 matrix, h is a vector of the 8 unknown homography parameters, and b is a 2n vector of target coordinates. I used 8 point correspondences for each image pair, creating a 16 × 8 overdetermined system. The solution is computed using numpy's least-squares solver (lstsq), which minimizes ||Ah - b||<sup>2</sup>. The resulting 8 parameters are then reshaped into a 3×3 homography matrix with h33 = 1.
            </p>

            <h3>Point Correspondences</h3>
            <p>
                I manually selected 8 corresponding points between image pairs using matplotlib's ginput function for interactive point selection. Each point was carefully chosen on distinctive features visible in both images. Careful selection is crucial as small errors in point correspondences can significantly affect the recovered homography. The points are visualized with matching colors and numbers across both images.
            </p>
            
            <div class="comparison-grid">
                <div class="comparison-item">
                    <img src="AOutputs/A2.jpg" alt="Point Correspondences Visualized">
                    <p><strong>Corresponding Points Marked on Images</strong></p>
                </div>
            </div>

            <h3>Recovered Homography Matrix</h3>
            <p>The computed homography matrix H transforms points from the source image to the reference image coordinate system. Here's an example recovered homography:</p>
            
            <div class="equation-box">
H = [h11  h12  h13]
    [h21  h22  h23]
    [h31  h32  1.00]

Example values (normalized with h33 = 1):
    [1.234  0.156  -45.2]
    [0.098  1.187  -12.5]
    [0.0002 0.0003  1.00]
            </div>
        </div>

        <div class="card">
            <h2>A.3: Warp the Images</h2>
            <p>
                Once we have the homography matrix H, I implemented image warping using <strong>inverse warping</strong> to avoid holes in the output. The algorithm first computes the output image dimensions by transforming the four corners of the source image through H to find the bounding box. Then, for each pixel in the output image, I compute H<sup>-1</sup> to map the output coordinates back to source image coordinates and sample the source. The warped images include an alpha channel to track valid pixels.
            </p>

            <h3>Interpolation Methods</h3>
            <p>Since the inverse-mapped coordinates are typically not integers, I implemented two interpolation methods from scratch:</p>
            
            <ul>
                <li><strong>Nearest Neighbor:</strong> For each output pixel, I round the source coordinates to the nearest integer using standard rounding, then clamp to image boundaries to handle edge cases. This is fast (single pixel lookup) but produces blocky artifacts, especially along edges.</li>
                <li><strong>Bilinear Interpolation:</strong> For each output pixel, I compute the floor of source coordinates to get the top-left neighbor (x1, y1), then use the four surrounding pixels (x1, y1), (x2, y1), (x1, y2), (x2, y2). The interpolation weights are wx = x_src - x1 and wy = y_src - y1, and the final value is: (1-wx)(1-wy)×p11 + wx(1-wy)×p21 + (1-wx)wy×p12 + wx×wy×p22. This produces smoother results at the cost of 4 pixel lookups and floating-point arithmetic per output pixel.</li>
            </ul>

            <h3>Rectification</h3>
            <p>
                To test the homography and warping implementation, I performed rectification on two different images. For each, I manually selected 4 corner points of a rectangular object in the image, then computed a homography mapping those points to a 200×200 square. This effectively "straightens" the perspective-distorted rectangles.
            </p>

            <h4>Example 1: Rectangular Panel</h4>
            <div class="comparison-grid">
                <div class="comparison-item">
                    <img src="AOriginals/RectPanel.jpg" alt="Original Image">
                    <p><strong>Original Image (Perspective View)</strong></p>
                </div>
                <div class="comparison-item">
                    <img src="AOutputs/A3_nearest_neighbor.jpg" alt="Nearest Neighbor Rectification">
                    <p><strong>Rectified - Nearest Neighbor</strong></p>
                </div>
                <div class="comparison-item">
                    <img src="AOutputs/A3_bilinear.jpg" alt="Bilinear Rectification">
                    <p><strong>Rectified - Bilinear Interpolation</strong></p>
                </div>
            </div>

            <h4>Example 2: Additional Rectification</h4>
            <div class="comparison-grid">
                <div class="comparison-item">
                    <img src="AOutputs/A3_original_with_points_2.jpg" alt="Original Image 2">
                    <p><strong>Original Image (Perspective View)</strong></p>
                </div>
                <div class="comparison-item">
                    <img src="AOutputs/A3_nearest_neighbor_2.jpg" alt="Nearest Neighbor Rectification 2">
                    <p><strong>Rectified - Nearest Neighbor</strong></p>
                </div>
                <div class="comparison-item">
                    <img src="AOutputs/A3_bilinear_2.jpg" alt="Bilinear Rectification 2">
                    <p><strong>Rectified - Bilinear Interpolation</strong></p>
                </div>
            </div>

            <h3>Comparison and Discussion</h3>
            <p>
                <strong>Quality:</strong> Bilinear interpolation produces noticeably smoother results, especially visible along edges and in regions with fine detail. Nearest neighbor interpolation creates a "blocky" or pixelated appearance due to discrete rounding.
            </p>
            <p>
                <strong>Speed:</strong> Nearest neighbor is significantly faster as it only requires rounding and a single pixel lookup. Bilinear interpolation requires four pixel lookups and floating-point arithmetic for weighted averaging.
            </p>
            <p>
                <strong>Trade-off:</strong> For final high-quality results, bilinear interpolation is preferred. For real-time applications or quick previews, nearest neighbor may be acceptable.
            </p>
        </div>

        <div class="card">
            <h2>A.4: Blend the Images into a Mosaic</h2>
            <p>
                The final step is to combine multiple warped images into a seamless mosaic. Simply overlaying images creates harsh edges and discontinuities. Instead, I use <strong>weighted averaging</strong> (feathering) where the contribution of each image gradually decreases toward its edges.
            </p>

            <h3>Blending Strategy</h3>
            <p>My implementation uses a global canvas approach with Gaussian-weighted alpha blending:</p>
            <ol>
                <li><strong>Compute Global Bounding Box:</strong> Transform the corners of the first image through the homography H, then combine with the corners of the reference image (which remains unwarped) to determine the minimum and maximum x, y coordinates. This defines the mosaic canvas size.</li>
                
                <li><strong>Create Gaussian Alpha Masks:</strong> For each image, I generate an alpha mask with Gaussian falloff from the center. The mask is computed as α = exp(-d²/(2σ²)) where d is the distance from the image center and σ = min(height, width)/4. This creates smooth weight transitions that are 1.0 at the center and approach 0.0 at edges.</li>
                
                <li><strong>Place Reference Image:</strong> The second image (reference) is placed directly onto the global canvas at its natural position (accounting for any negative offsets in the bounding box), multiplied by its alpha mask.</li>
                
                <li><strong>Warp and Place First Image:</strong> The first image is warped using bilinear interpolation to align with the reference frame. The warped image has its own Gaussian alpha mask that is further multiplied by the binary alpha channel (indicating valid pixels). This weighted image is then added to the canvas.</li>
                
                <li><strong>Normalize by Total Alpha:</strong> Throughout the process, I accumulate the total alpha weights. The final mosaic is computed as: mosaic = Σ(image_i × α_i) / Σ(α_i), ensuring proper weighted averaging in overlap regions.</li>
                
                <li><strong>Post-processing:</strong> A small Gaussian blur (σ=0.5, kernel size 3×3) is applied to further smooth any remaining artifacts at blend boundaries.</li>
            </ol>

            <h3>Mosaic Results</h3>
            
            <div class="mosaic-section">
                <h4>Mosaic 1: TV Screen Panorama</h4>
                <p>
                    This mosaic combines two images of a TV screen captured by rotating the camera. The images were warped using computed homographies and blended using distance-weighted averaging.
                </p>
                
                <div class="source-images">
                    <div>
                        <img src="AOutputs/A4_tvmosaic1_original.jpg" alt="Source 1">
                        <p>Source Image 1</p>
                    </div>
                    <div>
                        <img src="AOutputs/A4_tvmosaic2_reference.jpg" alt="Source 2">
                        <p>Source Image 2 (Reference)</p>
                    </div>
                </div>

                <div>
                    <img src="AOutputs/A4_tvmosaic1_warped.jpg" alt="Warped Image" class="full-width-image">
                    <p style="text-align: center;"><strong>Image 1 Warped to Reference Frame</strong></p>
                </div>

                <div>
                    <img src="AOutputs/A4_final_mosaic.jpg" alt="Final Mosaic" class="full-width-image">
                    <p style="text-align: center;"><strong>Final Blended Mosaic</strong></p>
                </div>
            </div>

            <div class="mosaic-section">
                <h4>Mosaic 1: Processing Array</h4>
                <p>
                    This visualization shows the step-by-step process of creating the mosaic, including intermediate warping stages and the blending process.
                </p>
                <img src="AOutputs/A4_mosaic_array.jpg" alt="Mosaic Processing Steps" class="full-width-image">
            </div>

            <div class="mosaic-section">
                <h4>Mosaic 2: Interior Scene Panorama</h4>
                <p>
                    This mosaic combines two images of an interior scene, demonstrating the algorithm's ability to handle complex textures and varying lighting conditions.
                </p>
                
                <div class="source-images">
                    <div>
                        <img src="AOutputs/A4_mosaic2_image1_original.jpg" alt="Source 1">
                        <p>Source Image 1</p>
                    </div>
                    <div>
                        <img src="AOutputs/A4_mosaic2_image2_reference.jpg" alt="Source 2">
                        <p>Source Image 2 (Reference)</p>
                    </div>
                </div>

                <div>
                    <img src="AOutputs/A4_mosaic2_image1_warped.jpg" alt="Warped Image" class="full-width-image">
                    <p style="text-align: center;"><strong>Image 1 Warped to Reference Frame</strong></p>
                </div>

                <div>
                    <img src="AOutputs/A4_mosaic2_final.jpg" alt="Final Mosaic" class="full-width-image">
                    <p style="text-align: center;"><strong>Final Blended Mosaic</strong></p>
                </div>
            </div>

            <div class="mosaic-section">
                <h4>Mosaic 2: Processing Array</h4>
                <p>
                    Step-by-step visualization of the interior scene mosaic creation process.
                </p>
                <img src="AOutputs/A4_mosaic2_array.jpg" alt="Mosaic 2 Processing Steps" class="full-width-image">
            </div>

            <div class="mosaic-section">
                <h4>Mosaic 3: Facade Panorama</h4>
                <p>
                    This mosaic stitches together images of a building facade, demonstrating the algorithm's ability to handle architectural features with strong geometric patterns.
                </p>
                
                <div class="source-images">
                    <div>
                        <img src="AOutputs/A4_mosaic3_image1_original.jpg" alt="Source 1">
                        <p>Source Image 1</p>
                    </div>
                    <div>
                        <img src="AOutputs/A4_mosaic3_image2_reference.jpg" alt="Source 2">
                        <p>Source Image 2 (Reference)</p>
                    </div>
                </div>

                <div>
                    <img src="AOutputs/A4_mosaic3_image1_warped.jpg" alt="Warped Image" class="full-width-image">
                    <p style="text-align: center;"><strong>Image 1 Warped to Reference Frame</strong></p>
                </div>

                <div>
                    <img src="AOutputs/A4_mosaic3_final.jpg" alt="Final Mosaic" class="full-width-image">
                    <p style="text-align: center;"><strong>Final Blended Mosaic</strong></p>
                </div>
            </div>

            <div class="mosaic-section">
                <h4>Mosaic 3: Processing Array</h4>
                <p>
                    Step-by-step visualization of the facade mosaic creation process.
                </p>
                <img src="AOutputs/A4_mosaic3_array.jpg" alt="Mosaic 3 Processing Steps" class="full-width-image">
            </div>

            <h3>Blending Implementation Details</h3>
            <p>
                The key to seamless mosaics is the Gaussian alpha mask strategy. By using exponential falloff (Gaussian) rather than linear, the blending appears more natural because the weight transitions follow a smooth, continuous curve. The choice of σ = min(height, width)/4 ensures the mask has significant influence across the entire image while maintaining the strongest weights at the center.
            </p>
            <p>
                The weighted averaging approach (dividing by total accumulated alpha) is crucial in overlap regions. Without this normalization, overlap areas would be brighter than non-overlap regions. The division ensures that regardless of how many images contribute to a pixel, the final intensity is correctly averaged.
            </p>
            <p>
                The final Gaussian blur pass helps eliminate any subtle discontinuities at blend boundaries. Using a small kernel (3×3 with σ=0.5) provides just enough smoothing without noticeably degrading image sharpness. For scenes with more challenging lighting variations or more complex blending scenarios, Laplacian pyramid blending could be employed to separately handle low and high-frequency components.
            </p>
        </div>

        <div class="card">
            <h2>B.1: Harris Corner Detection & Adaptive Non-Maximal Suppression</h2>
            <p>
                I implemented Harris corner detection at a single scale with an edge discard threshold of 20 pixels. The raw Harris detector identifies hundreds of interest points, but many cluster in high-texture regions.
            </p>

            <h3>Harris Corner Detection Results</h3>
            <div class="comparison-grid">
                <div class="comparison-item">
                    <img src="BOutputs/B1_1_harris_all.jpg" alt="Harris corners detected">
                    <p><strong>Example 1: All Harris Corners</strong></p>
                </div>
                <div class="comparison-item">
                    <img src="BOutputs/B1_2_harris_all.jpg" alt="Harris corners detected">
                    <p><strong>Example 2: All Harris Corners</strong></p>
                </div>
            </div>

            <h3>Adaptive Non-Maximal Suppression</h3>
            <p>
                I applied ANMS to select a spatially distributed subset of 500 corners. Each corner's suppression radius is computed as the minimum distance to any stronger corner (score > 0.9 × the current corner's score). Selecting the 500 corners with the largest suppression radii produces a well-distributed set suitable for homography estimation.
            </p>

            <div class="comparison-grid">
                <div class="comparison-item">
                    <img src="BOutputs/B1_1_harris_anms.jpg" alt="ANMS corners selected">
                    <p><strong>Example 1: ANMS (n=500)</strong></p>
                </div>
                <div class="comparison-item">
                    <img src="BOutputs/B1_2_harris_anms.jpg" alt="ANMS corners selected">
                    <p><strong>Example 2: ANMS (n=500)</strong></p>
                </div>
            </div>
        </div>

        <div class="card">
            <h2>B.2: Feature Descriptor Extraction</h2>
            <p>
                For each corner selected by ANMS, I extracted feature descriptors by sampling an 8×8 grid at 5-pixel spacing within a 40×40 window (after Gaussian blur, σ=1.0), then applying bias/gain normalization (subtract mean, divide by standard deviation). This produces 64-dimensional descriptors invariant to affine intensity changes.
            </p>

            <h3>Extracted Descriptors</h3>
            <div class="comparison-grid">
                <div class="comparison-item">
                    <img src="BOutputs/B2_1_descriptor_context.jpg" alt="40x40 context windows">
                    <p><strong>Example 1: 40×40 Sampling Windows</strong></p>
                </div>
                <div class="comparison-item">
                    <img src="BOutputs/B2_1_descriptor_patches.jpg" alt="8x8 feature descriptors">
                    <p><strong>Example 1: 8×8 Normalized Descriptors</strong></p>
                </div>
            </div>

            <div class="comparison-grid">
                <div class="comparison-item">
                    <img src="BOutputs/B2_2_descriptor_context.jpg" alt="40x40 context windows">
                    <p><strong>Example 2: 40×40 Sampling Windows</strong></p>
                </div>
                <div class="comparison-item">
                    <img src="BOutputs/B2_2_descriptor_patches.jpg" alt="8x8 feature descriptors">
                    <p><strong>Example 2: 8×8 Normalized Descriptors</strong></p>
                </div>
            </div>
        </div>

        <div class="card">
            <h2>B.3: Feature Matching</h2>
            <p>
                I implemented feature matching using Lowe's ratio test with symmetric (mutual nearest neighbor) constraints. For each descriptor in image 1, I compute the ratio between its nearest and second-nearest neighbor distances in image 2, accepting matches only when the ratio is below 0.75.
            </p>

            <h3>Feature Matches</h3>
            <div class="comparison-grid">
                <div class="comparison-item">
                    <img src="BOutputs/B3_matches.jpg" alt="Feature matches between image pair">
                    <p><strong>Matched Features Between Image Pair</strong><br>Symmetric Lowe ratio < 0.75</p>
                </div>
            </div>
        </div>

        <div class="card">
            <h2>B.4: RANSAC & Automatic Image Stitching</h2>
            <p>
                I implemented 4-point RANSAC from scratch to compute robust homographies. Each iteration randomly samples 4 matched feature pairs, computes a candidate homography, and counts inliers (matches with reprojection error < 3.0 pixels). After 5000 iterations, the homography with the most inliers is refined using all inlier correspondences via least-squares.
            </p>
            <p>
                The automatic stitching pipeline combines all previous steps: Harris corner detection, ANMS, descriptor extraction, feature matching (symmetric Lowe ratio < 0.75, top 500 matches), RANSAC homography estimation, mosaic blending. Below are comparisons of manually-stitched mosaics from Part A with automatically-stitched versions using the same image pairs.
            </p>

            <h3>Mosaic Comparisons: Manual vs. Automatic</h3>
            
            <div class="mosaic-section">
                <h4>Mosaic 1: Manual vs. Automatic</h4>
                <div class="comparison-grid">
                    <div class="comparison-item">
                        <img src="AOutputs/A4_final_mosaic.jpg" alt="Manual mosaic 1">
                        <p><strong>Manual Stitching (Part A)</strong></p>
                    </div>
                    <div class="comparison-item">
                        <img src="BOutputs/B4_mosaic1_final.jpg" alt="Automatic mosaic 1">
                        <p><strong>Automatic Stitching (Part B)</strong></p>
                    </div>
                </div>
            </div>

            <div class="mosaic-section">
                <h4>Mosaic 2: Manual vs. Automatic</h4>
                <div class="comparison-grid">
                    <div class="comparison-item">
                        <img src="AOutputs/A4_mosaic2_final.jpg" alt="Manual mosaic 2">
                        <p><strong>Manual Stitching (Part A)</strong></p>
                    </div>
                    <div class="comparison-item">
                        <img src="BOutputs/B4_mosaic2_final.jpg" alt="Automatic mosaic 2">
                        <p><strong>Automatic Stitching (Part B)</strong></p>
                    </div>
                </div>
            </div>

            <div class="mosaic-section">
                <h4>Mosaic 3: Manual vs. Automatic</h4>
                <div class="comparison-grid">
                    <div class="comparison-item">
                        <img src="AOutputs/A4_mosaic3_final.jpg" alt="Manual mosaic 3">
                        <p><strong>Manual Stitching (Part A)</strong></p>
                    </div>
                    <div class="comparison-item">
                        <img src="BOutputs/B4_mosaic3_final.jpg" alt="Automatic mosaic 3">
                        <p><strong>Automatic Stitching (Part B)</strong></p>
                    </div>
                </div>
            </div>

            <h3>Results & Discussion</h3>
            <p>
                The automatic stitching pipeline successfully produces mosaics comparable to and even better than manual correspondence selection. RANSAC effectively filters outliers from the feature matching stage, identifying robust inlier sets from the initial matched features. The symmetric Lowe ratio constraint significantly improves match quality compared to one-way matching. Automatic feature detection eliminates the tedious manual point selection process while improving alignment accuracy.
            </p>
        </div>

    <a href="../index.html" class="button">Back to Home</a>
</body>
</html>
